
Sampler - produces denormalized text based off given model
	Temperature only works from 0 to 1

tweetfile maker - picks model and produces a text file for it
	DONE -trims tweets to proper length
	DONE -enters proper length tweets into file
	DONE -produces text endings from sampler until the tweet is a complete sentence
	DONE - loops through entire given file
	DONE - Corrects capitlization after punctuations and checks for duplicates
	DONE - leaves a specific 6 character number next to each tweet for reference

test sample - makes a short sample from a training checkpoint
	
tweet image scrapper
	-optionally take two words given or use a tweetfile
	-checks tweetfile to see if a file already exists for the given tweet
	-randomly picks a word from the tweet or given text and uses that to search on Google Images Search API
	-saves file named after the tweet's 6 character number to folder named after the tweetfile

tweetimage prepare
	-takes a tweetfile's 6 character code
	-finds associated image
	-adjusts photo to be labelled in whatever way
	-saves photo
	
tweet - picks the first available tweet from specificed tweetfile
	-double checks it is 140 characters
	-picks image if prompted from image directory associated with tweetfile

compound tweet system
	-takes tweets from different files and combines them
	-checks for proper length, and trims end if it's too long
	-performs basic three word ending task from tweetfile maker to make sure each file ends
	-posts to Istagram and facebook

Training Text Parser:
	-Takes a text file
	-Replaces all common mistakes, i.e. replace \r with \n, delete all \n\n
	-Puts list into a list broken on \n
	-trims list to remove empty list items
	-finds and adds \n for all dialog lines
	-Saves to a new file
	
Trainer :
	TO FIND AVERAGE: ADD ALL ITEMS TOGETHER AND THEN DIVIDE BY TOTAL NUMBER
	trains a model off a given text corpus, making suggested changes to train.lua commands based off file
	data and recommendations from char-rnn
	
	< 1mb is considdered very small
	if data is <2 mb, increase RNN size
	
	if data set is small, reduce batch size and sequence length

	
	
	Best model has lowest Validation loss.
	in lm_lstm_epoch0.95_2.0681.t7
	2.0681.t7 is the validation loss amount
	
	Compare this number to the training loss
		if training loss is much less than validation loss, network is overfitting
			decrease network size or increase dropout
		if training loss is about equal to validation, network is underfitting
			increase number of layers or number of neurons per layer
			
	Always use num_layers 2 oe 3
	
	Size of data set (1 mb ~= 100000 characters) should be same order of magnitude
		100 md dataset with default batch and sequence settings works with 150K parameters
		100,000,000 characters > .15 million characters
		
	
	Twitter bot design
1. Take basis corpus
2. strip first three words and put them into a list
3. strip last three words and put them into a list
4. Train character models on these two
5. Train word model on long form.

6. Render out a large source text from long form model and the first characters from the first three words

7. Spilt resulting text into 140 character strings that start at the nearest previous whitespace.

8. Start ending loop for character:
	If the final character is a puncuation and it's within the character limit, stop.
	
	If it doesn't end with a punctuation, grab last word and use it as a seed for the ending generation. Adjust the seed number and check for punctation endings and length of 140 characters 5 times.
	
	If the tweet still doesn't end with punctuation, cut back to the previous 2 words, grab last word and use it as a seed for the ending generation, attempt adjusting the seed number again and check for punctation endings and length of 140 characters 15 more times.
	
	If the sentence still doesn't end with a punctuation, check length is under 139 characters and then end with a punctiation.
		If the file is still too long, hard truncate a word and add a punctation.

